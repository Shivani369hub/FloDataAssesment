{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean the orders data\"\"\"\n",
    "    # Clean dollar amounts\n",
    "    df['$ sale'] = df['$ sale'].str.replace('$', '').str.replace(',', '').str.strip().astype(float)\n",
    "    \n",
    "    # Clean state abbreviations (convert to uppercase and fix inconsistencies)\n",
    "    df['state'] = df['state'].str.upper().str.strip()\n",
    "    \n",
    "    # Clean zip codes\n",
    "    df['Zip'] = df['Zip'].astype(str).str.strip()\n",
    "\n",
    "    # dropping the duplicates from order data assuming that order numbers can not be duplicated\n",
    "    df.drop_duplicates(subset=['order_number'], inplace=True)\n",
    "    \n",
    "    # Rename columns to match our database schema\n",
    "    df = df.rename(columns={\n",
    "        'date': 'order_date',\n",
    "        '$ sale': 'order_amount',\n",
    "        'Zip': 'zip_code'\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database():\n",
    "    \"\"\"Initialize the database\"\"\"\n",
    "    conn = sqlite3.connect('sales_orders.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS orders (\n",
    "            order_number TEXT PRIMARY KEY,\n",
    "            order_date TEXT,\n",
    "            city TEXT,\n",
    "            state TEXT,\n",
    "            zip_code TEXT,\n",
    "            order_amount REAL,\n",
    "            ip_address TEXT,\n",
    "            processed INTEGER DEFAULT 0\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS ip_cache (\n",
    "            ip_address TEXT PRIMARY KEY,\n",
    "            city TEXT,\n",
    "            state TEXT,\n",
    "            zip_code TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_orders(conn, orders_file):\n",
    "    \"\"\"Load and clean orders data\"\"\"\n",
    "    df = pd.read_csv(orders_file)\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Get existing order numbers\n",
    "    existing_orders = pd.read_sql('SELECT DISTINCT order_number FROM orders', conn)\n",
    "    existing_numbers = existing_orders['order_number'].tolist()\n",
    "    \n",
    "    # Filter out duplicates\n",
    "    new_orders = df[~df['order_number'].isin(existing_numbers)]\n",
    "    \n",
    "    if not new_orders.empty:\n",
    "        # Insert in chunks to handle potential errors\n",
    "        chunk_size = 100\n",
    "        for i in range(0, len(new_orders), chunk_size):\n",
    "            chunk = new_orders.iloc[i:i + chunk_size]\n",
    "            try:\n",
    "                chunk.to_sql('orders', conn, if_exists='append', index=False)\n",
    "                print(f\"Added {len(chunk)} new orders (chunk {i//chunk_size + 1})\")\n",
    "            except sqlite3.IntegrityError as e:\n",
    "                print(f\"Error inserting chunk {i//chunk_size + 1}: {str(e)}\")\n",
    "    else:\n",
    "        print(\"No new orders to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_ip(ip):\n",
    "    \"\"\"Get location from IP using free API\"\"\"\n",
    "    if not ip or pd.isna(ip) or ip.strip() == '':\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(f'http://ip-api.com/json/{ip}?fields=status,message,city,region,zip')\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'success':\n",
    "            return {\n",
    "                'city': data.get('city'),\n",
    "                'state': data.get('region'),\n",
    "                'zip_code': data.get('zip')\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing IP {ip}: {str(e)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ip_addresses(conn, ip_file):\n",
    "    \"\"\"Ultra-fast IP processing with bulk operations\"\"\"\n",
    "    # 1. Load data with optimized parameters\n",
    "    ip_df = pd.read_csv(ip_file, usecols=['order_number', 'ip_address'])\n",
    "    \n",
    "    # 2. Fast filtering of invalid IPs\n",
    "    mask = ip_df['ip_address'].notna() & (ip_df['ip_address'].str.strip() != '')\n",
    "    ip_df = ip_df[mask].copy()\n",
    "    ip_df['ip_address'] = ip_df['ip_address'].str.strip()\n",
    "\n",
    "    ip_df['ip_address'] = ip_df['ip_address'].str.rstrip(':')\n",
    "    \n",
    "    if ip_df.empty:\n",
    "        print(\"No valid IP addresses to process\")\n",
    "        return\n",
    "\n",
    "    # 3. Get cached IPs in one query (faster than DISTINCT)\n",
    "    cached_ips = pd.read_sql(\n",
    "        'SELECT ip_address FROM ip_cache GROUP BY ip_address', \n",
    "        conn\n",
    "    )['ip_address'].values\n",
    "    \n",
    "    # 4. Filter out cached IPs using numpy (much faster than isin)\n",
    "    import numpy as np\n",
    "    cached_set = set(cached_ips)\n",
    "    uncached_mask = ~np.isin(ip_df['ip_address'].values, list(cached_set))\n",
    "    uncached_df = ip_df[uncached_mask]\n",
    "    \n",
    "    if uncached_df.empty:\n",
    "        print(\"All IPs already cached\")\n",
    "        return\n",
    "\n",
    "    # 5. Process unique IPs in bulk\n",
    "    unique_ips = uncached_df['ip_address'].unique()\n",
    "    ip_to_orders = uncached_df.groupby('ip_address')['order_number'].apply(list).to_dict()\n",
    "    \n",
    "    # 6. Prepare bulk inserts and updates\n",
    "    cache_inserts = []\n",
    "    order_updates = []\n",
    "    \n",
    "    for ip in unique_ips:\n",
    "        location = get_location_from_ip(ip)\n",
    "        if location and location.get('zip_code'):\n",
    "            # Prepare cache insert\n",
    "            cache_inserts.append((\n",
    "                ip,\n",
    "                location['city'],\n",
    "                location['state'],\n",
    "                location['zip_code']\n",
    "            ))\n",
    "            \n",
    "            # Prepare order updates for all orders with this IP\n",
    "            for order_num in ip_to_orders[ip]:\n",
    "                order_updates.append((\n",
    "                    location['city'],\n",
    "                    location['state'],\n",
    "                    location['zip_code'],\n",
    "                    ip,\n",
    "                    order_num\n",
    "                ))\n",
    "    \n",
    "    # 7. Execute bulk operations\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print('Started Inserting')\n",
    "    # Bulk insert to cache\n",
    "    if cache_inserts:\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO ip_cache (ip_address, city, state, zip_code)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', cache_inserts)\n",
    "    \n",
    "    # Bulk update orders\n",
    "    if order_updates:\n",
    "        cursor.executemany('''\n",
    "            UPDATE orders \n",
    "            SET city = ?, state = ?, zip_code = ?, ip_address = ?, processed = 1\n",
    "            WHERE order_number = ? AND processed = 0\n",
    "        ''', order_updates)\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"Processed {len(unique_ips)} IPs, updated {len(order_updates)} orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_enriched_orders(conn, output_file):\n",
    "    \"\"\"Export processed orders to CSV\"\"\"\n",
    "    df = pd.read_sql('''\n",
    "        SELECT \n",
    "            order_number,\n",
    "            order_date,\n",
    "            city,\n",
    "            state,\n",
    "            zip_code,\n",
    "            order_amount\n",
    "        FROM orders \n",
    "        WHERE processed = 1\n",
    "    ''', conn)\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Exported {len(df)} orders to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = setup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new orders to add\n"
     ]
    }
   ],
   "source": [
    "load_orders(conn, 'orders_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Inserting\n",
      "Processed 200 IPs, updated 194 orders\n"
     ]
    }
   ],
   "source": [
    "process_ip_addresses(conn, 'ip_addresses_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 194 orders to enriched_orders.csv\n"
     ]
    }
   ],
   "source": [
    "export_enriched_orders(conn, 'enriched_orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sales_report(conn, state, year, output_file):\n",
    "    \"\"\"Generate quarterly sales report\"\"\"\n",
    "    # Get orders data\n",
    "    query = '''\n",
    "        SELECT \n",
    "            order_date,\n",
    "            city,\n",
    "            order_amount\n",
    "        FROM orders\n",
    "        WHERE state = ? AND substr(order_date, 1, 4) = ?\n",
    "            AND city IS NOT NULL AND city != ''\n",
    "    '''\n",
    "    df = pd.read_sql(query, conn, params=(state, str(year)))\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"No data found for {state} in {year}\")\n",
    "        return\n",
    "    \n",
    "    # Process dates and quarters\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    df['quarter'] = df['order_date'].dt.quarter\n",
    "    \n",
    "    # Group by city and quarter\n",
    "    grouped = df.groupby(['city', 'quarter'])['order_amount'].sum().unstack()\n",
    "    \n",
    "    # Create report in the required format\n",
    "    report_data = []\n",
    "    \n",
    "    # Add header\n",
    "    report_data.append([f\"{year} State: {state}\", \"\", \"\"])\n",
    "    \n",
    "    for q in range(1, 5):\n",
    "        report_data.append([f\"Q{q}\", \"City\", \"Total Sales\"])\n",
    "        \n",
    "        if q in grouped.columns:\n",
    "            quarter_data = grouped[q].reset_index()\n",
    "            quarter_data.columns = ['City', 'Total Sales']\n",
    "            quarter_data = quarter_data.sort_values('City')\n",
    "            \n",
    "            for _, row in quarter_data.iterrows():\n",
    "                report_data.append([\"\", row['City'], row['Total Sales']])\n",
    "        else:\n",
    "            print(f\"No data for Q{q}\")\n",
    "        \n",
    "        report_data.append([\"\", \"\", \"\"])  # Empty row between quarters\n",
    "    \n",
    "    # Create DataFrame and export to Excel\n",
    "    report_df = pd.DataFrame(report_data, columns=[\"A\", \"B\", \"C\"])\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        report_df.to_excel(\n",
    "            writer, \n",
    "            sheet_name=\"Sales Report\", \n",
    "            index=False, \n",
    "            header=False\n",
    "        )\n",
    "        \n",
    "        # Adjust column widths\n",
    "        worksheet = writer.sheets[\"Sales Report\"]\n",
    "        worksheet.column_dimensions['A'].width = 15\n",
    "        worksheet.column_dimensions['B'].width = 15\n",
    "        worksheet.column_dimensions['C'].width = 15\n",
    "    \n",
    "    print(f\"Report generated: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for Q4\n",
      "Report generated: IL_state_sales_report_2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "generate_sales_report(conn, 'IL', 2024, 'IL_state_sales_report_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asttokens==3.0.0\n",
      "certifi==2025.1.31\n",
      "charset-normalizer==3.4.1\n",
      "comm==0.2.2\n",
      "debugpy==1.8.13\n",
      "decorator==5.2.1\n",
      "et_xmlfile==2.0.0\n",
      "exceptiongroup==1.2.2\n",
      "executing==2.2.0\n",
      "idna==3.10\n",
      "ipykernel==6.29.5\n",
      "ipython==8.34.0\n",
      "jedi==0.19.2\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.7.2\n",
      "matplotlib-inline==0.1.7\n",
      "nest-asyncio==1.6.0\n",
      "numpy==2.2.4\n",
      "openpyxl==3.1.5\n",
      "packaging==24.2\n",
      "pandas==2.2.3\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "platformdirs==4.3.7\n",
      "prompt_toolkit==3.0.50\n",
      "psutil==7.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "Pygments==2.19.1\n",
      "python-dateutil==2.9.0.post0\n",
      "pytz==2025.2\n",
      "pyzmq==26.3.0\n",
      "requests==2.32.3\n",
      "six==1.17.0\n",
      "stack-data==0.6.3\n",
      "tornado==6.4.2\n",
      "traitlets==5.14.3\n",
      "typing_extensions==4.13.0\n",
      "tzdata==2025.2\n",
      "urllib3==2.3.0\n",
      "wcwidth==0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
